{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DydO5o-djdGT"
   },
   "source": [
    "#Mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2272,
     "status": "ok",
     "timestamp": 1760772425026,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "CwVXszYVHEqZ",
    "outputId": "05eb69b6-bb48-4beb-dd16-64e3cbc39588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "‚úÖ Google Drive mounted successfully!\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "file_path = '/content/drive/My Drive/'\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRpxwYZNJ7T7"
   },
   "source": [
    "\n",
    "#install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 12021,
     "status": "ok",
     "timestamp": 1760772437045,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "te2Ul8tJEzwO",
    "outputId": "2f81e782-2cb8-429a-8156-947f5b530f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.119.0)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.37.0)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
      "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (0.0.20)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: tldextract in /usr/local/lib/python3.12/dist-packages (5.3.0)\n",
      "Requirement already satisfied: whois in /usr/local/lib/python3.12/dist-packages (1.20240129.2)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.48.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.10)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.0)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.0.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.20.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "‚úÖ Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn nest-asyncio python-multipart openai requests tensorflow scikit-learn tldextract whois\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4UzI6wGKKRS"
   },
   "source": [
    "#import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8729,
     "status": "ok",
     "timestamp": 1760772445777,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "fFgVh7FIFT8d",
    "outputId": "2f1bdb64-57fa-43ca-89cb-061a8a48f604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import HTMLResponse\n",
    "import requests\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "import joblib\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "import os\n",
    "import warnings\n",
    "from getpass import getpass\n",
    "from threading import Thread\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pe4jGjCOKVQq"
   },
   "source": [
    "#setup colab public url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1760772446330,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "UVxLf2JPFb4w",
    "outputId": "fab681de-45f1-4165-b69b-dde444df488b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Colab Public URL: https://8000-gpu-t4-s-21wrve7j1hugn-c.us-east1-1.prod.colab.dev\n",
      "‚úÖ Colab setup complete!\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "# Get Colab's public URL\n",
    "try:\n",
    "    from google.colab.output import eval_js\n",
    "    colab_public_url = eval_js(\"google.colab.kernel.proxyPort(8000)\")\n",
    "    print(f\"üåê Colab Public URL: {colab_public_url}\")\n",
    "except:\n",
    "    colab_public_url = \"http://localhost:8000\"\n",
    "    print(\"üîß Using localhost\")\n",
    "\n",
    "print(\"‚úÖ Colab setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7YANwRQUYdb"
   },
   "source": [
    "#Setup fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1760772446714,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "7GkCDwaMUZZu",
    "outputId": "eaada31c-4c29-4afa-898f-6762a98a3589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FastAPI app initialized!\n"
     ]
    }
   ],
   "source": [
    "app = FastAPI(title=\"Phishing URL Analyzer - Colab Version\")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ FastAPI app initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGt0tOfcUgi8"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4810,
     "status": "ok",
     "timestamp": 1760772451527,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "voI8FnDMFf1W",
    "outputId": "394dda8f-6940-4acf-f83b-e98454790142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Enter your OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "‚úÖ OpenAI client configured!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    print(\"‚úÖ OpenAI client configured!\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è No OpenAI API key provided. LLM features will be disabled.\")\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2bJocDtGXSs"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rvj_v57NUz-g"
   },
   "source": [
    "#Check files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1760772451537,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "004hme5hU0tQ",
    "outputId": "8e27d7be-6eeb-432a-d850-1a407c404123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Please upload your model files:\n",
      "   - scaler.joblib\n",
      "   - tokenizer.joblib\n",
      "   - labelencoder.joblib\n",
      "   - model.keras\n",
      "\n",
      "üí° If you don't have these files, the system will use demo predictions.\n",
      "‚úÖ Found files: ['/content/drive/MyDrive/Project/phissing_research/LLMAntiPhish/models/scaler.joblib', '/content/drive/MyDrive/Project/phissing_research/LLMAntiPhish/models/tokenizer.joblib', '/content/drive/MyDrive/Project/phissing_research/LLMAntiPhish/models/label_encoder.joblib', '/content/drive/MyDrive/Project/phissing_research/LLMAntiPhish/models/bilstm_model.keras']\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÅ Please upload your model files:\")\n",
    "print(\"   - scaler.joblib\")\n",
    "print(\"   - tokenizer.joblib\")\n",
    "print(\"   - labelencoder.joblib\")\n",
    "print(\"   - model.keras\")\n",
    "print()\n",
    "print(\"üí° If you don't have these files, the system will use demo predictions.\")\n",
    "\n",
    "existing_files = []\n",
    "for file in ['scaler path',\n",
    "             'tokenizer path',\n",
    "             'label_encoder path',\n",
    "             'model path']:\n",
    "    if os.path.exists(file):\n",
    "        existing_files.append(file)\n",
    "\n",
    "if existing_files:\n",
    "    print(f\"‚úÖ Found files: {existing_files}\")\n",
    "else:\n",
    "    print(\"üîß No model files found. Using demo mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdUcNpYVL03"
   },
   "source": [
    "#Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3224,
     "status": "ok",
     "timestamp": 1760772454763,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "oJFljG4qVMOW",
    "outputId": "513c2250-c844-42eb-a96f-59ccfbce18a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading REAL models from Google Drive...\n",
      "‚úÖ Scaler loaded successfully!\n",
      "‚úÖ Tokenizer loaded successfully!\n",
      "‚úÖ LabelEncoder loaded successfully!\n",
      "‚úÖ BiLSTM Model loaded successfully!\n",
      "\n",
      "üéâ SUCCESS: All REAL models loaded! Using trained BiLSTM model.\n",
      "üìä Model will provide consistent and accurate predictions.\n",
      "\n",
      "‚úÖ Model components setup complete!\n"
     ]
    }
   ],
   "source": [
    "maxlen = 50\n",
    "\n",
    "# Custom Attention Layer\n",
    "@register_keras_serializable()\n",
    "class Attention(Layer):\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        self.u = self.add_weight(shape=(input_shape[-1], 1),\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        u_it = tf.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "        a_it = tf.nn.softmax(tf.tensordot(u_it, self.u, axes=1), axis=1)\n",
    "        return tf.reduce_sum(x * a_it, axis=1)\n",
    "\n",
    "#Fallback model\n",
    "class FallbackModel:\n",
    "    def __init__(self):\n",
    "        self.pattern_probs = {\n",
    "            # Safe domains\n",
    "            'google.com': [0.85, 0.15],\n",
    "            'facebook.com': [0.82, 0.18],\n",
    "            'github.com': [0.88, 0.12],\n",
    "            'amazon.com': [0.80, 0.20],\n",
    "            'microsoft.com': [0.83, 0.17],\n",
    "            'example.com': [0.75, 0.25],\n",
    "            # Suspicious patterns\n",
    "            'paypal': [0.25, 0.75],\n",
    "            'login': [0.35, 0.65],\n",
    "            'verify': [0.30, 0.70],\n",
    "            'banking': [0.28, 0.72],\n",
    "            'secure': [0.32, 0.68],\n",
    "            # Default\n",
    "            'default': [0.60, 0.40]\n",
    "        }\n",
    "\n",
    "    def predict(self, x):\n",
    "        if hasattr(self, 'last_url'):\n",
    "            url = self.last_url.lower()\n",
    "\n",
    "            for pattern, prob in self.pattern_probs.items():\n",
    "                if pattern in url and pattern != 'default':\n",
    "                    return np.array([prob])\n",
    "\n",
    "            suspicious_keywords = ['paypal', 'login', 'verify', 'banking', 'secure', 'account', 'password']\n",
    "            safe_keywords = ['google', 'facebook', 'amazon', 'microsoft', 'github', 'official']\n",
    "\n",
    "            if any(suspicious in url for suspicious in suspicious_keywords):\n",
    "                return np.array([[0.3, 0.7]])\n",
    "            elif any(safe in url for safe in safe_keywords):\n",
    "                return np.array([[0.8, 0.2]])\n",
    "\n",
    "        return np.array([[0.6, 0.4]])\n",
    "\n",
    "scaler, tokenizer, le, model = None, None, None, None\n",
    "model_files = {\n",
    "    'scaler': 'scaler path',\n",
    "    'tokenizer': 'tokenzizer path',\n",
    "    'labelencoder': 'label_encoder path',\n",
    "    'model': 'model path'\n",
    "}\n",
    "\n",
    "print(\"üîç Loading REAL models from Google Drive...\")\n",
    "\n",
    "try:\n",
    "    # Load Scaler\n",
    "    if os.path.exists(model_files['scaler']):\n",
    "        scaler = joblib.load(model_files['scaler'])\n",
    "        print(\"‚úÖ Scaler loaded successfully!\")\n",
    "    else:\n",
    "        print(\"‚ùå Scaler file not found\")\n",
    "\n",
    "    # Load Tokenizer\n",
    "    if os.path.exists(model_files['tokenizer']):\n",
    "        tokenizer = joblib.load(model_files['tokenizer'])\n",
    "        print(\"‚úÖ Tokenizer loaded successfully!\")\n",
    "    else:\n",
    "        print(\"‚ùå Tokenizer file not found\")\n",
    "\n",
    "    # Load LabelEncoder\n",
    "    if os.path.exists(model_files['labelencoder']):\n",
    "        le = joblib.load(model_files['labelencoder'])\n",
    "        print(\"‚úÖ LabelEncoder loaded successfully!\")\n",
    "    else:\n",
    "        print(\"‚ùå LabelEncoder file not found\")\n",
    "\n",
    "    # Load Main Model\n",
    "    if os.path.exists(model_files['model']):\n",
    "        model = load_model(model_files['model'], custom_objects={\"Attention\": Attention})\n",
    "        print(\"‚úÖ BiLSTM Model loaded successfully!\")\n",
    "    else:\n",
    "        print(\"‚ùå Model file not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error loading model files: {e}\")\n",
    "\n",
    "# Check if all models loaded successfully\n",
    "if all([scaler, tokenizer, le, model]):\n",
    "    print(\"\\nüéâ SUCCESS: All REAL models loaded! Using trained BiLSTM model.\")\n",
    "    print(\"üìä Model will provide consistent and accurate predictions.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Some model files are missing. Using fallback model.\")\n",
    "    missing_components = []\n",
    "    if scaler is None: missing_components.append(\"scaler\")\n",
    "    if tokenizer is None: missing_components.append(\"tokenizer\")\n",
    "    if le is None: missing_components.append(\"labelencoder\")\n",
    "    if model is None: missing_components.append(\"model\")\n",
    "\n",
    "    print(f\"‚ùå Missing: {', '.join(missing_components)}\")\n",
    "    print(\"üîß Using fallback pattern-based model (consistent predictions)\")\n",
    "    model = FallbackModel()\n",
    "\n",
    "print(\"\\n‚úÖ Model components setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIYBOayYksVn"
   },
   "source": [
    "#Build FallbackModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1760772454766,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "d-uJY381-gnK"
   },
   "outputs": [],
   "source": [
    "class FallbackModel:\n",
    "    def __init__(self):\n",
    "        self.pattern_probs = {\n",
    "            # Safe domains\n",
    "            'google.com': [0.85, 0.15],\n",
    "            'facebook.com': [0.82, 0.18],\n",
    "            'github.com': [0.88, 0.12],\n",
    "            'amazon.com': [0.80, 0.20],\n",
    "            'microsoft.com': [0.83, 0.17],\n",
    "            'example.com': [0.75, 0.25],\n",
    "            # Suspicious patterns\n",
    "            'paypal': [0.25, 0.75],\n",
    "            'login': [0.35, 0.65],\n",
    "            'verify': [0.30, 0.70],\n",
    "            'banking': [0.28, 0.72],\n",
    "            'secure': [0.32, 0.68],\n",
    "            # Default\n",
    "            'default': [0.60, 0.40]\n",
    "        }\n",
    "\n",
    "    def predict_proper(self, url):\n",
    "        url_lower = url.lower()\n",
    "\n",
    "        for pattern, prob in self.pattern_probs.items():\n",
    "            if pattern in url_lower and pattern != 'default':\n",
    "                return np.array([prob])\n",
    "\n",
    "        suspicious_keywords = ['paypal', 'login', 'verify', 'banking', 'secure', 'account', 'password']\n",
    "        safe_keywords = ['google', 'facebook', 'amazon', 'microsoft', 'github', 'official']\n",
    "\n",
    "        if any(suspicious in url_lower for suspicious in suspicious_keywords):\n",
    "            return np.array([[0.3, 0.7]])\n",
    "        elif any(safe in url_lower for safe in safe_keywords):\n",
    "            return np.array([[0.8, 0.2]])\n",
    "\n",
    "        return np.array([[0.6, 0.4]])\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.array([[0.5, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9qkbwMikvxs"
   },
   "source": [
    "#Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1760772454815,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "B1-9h5VKVR26",
    "outputId": "84f580c1-75f9-49f3-9600-3bc994c390f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature extraction functions defined!\n"
     ]
    }
   ],
   "source": [
    "BRAND_KEYWORDS = [\"paypal\",\"apple\",\"amazon\",\"bank\",\"chase\",\"facebook\",\"meta\",\"google\",\"microsoft\",\n",
    "                  \"outlook\",\"office365\",\"instagram\",\"line\",\"kbank\",\"scb\",\"krungsri\",\"kplus\"]\n",
    "\n",
    "COMMON_TLDS = set([\n",
    " \"com\",\"net\",\"org\",\"info\",\"biz\",\"co\",\"io\",\"ai\",\"app\",\"edu\",\"gov\",\"mil\",\"ru\",\"de\",\"uk\",\"cn\",\"fr\",\"jp\",\"br\",\"in\",\"it\",\"es\",\"au\",\"nl\",\"se\",\"no\"\n",
    "])\n",
    "\n",
    "def parse_host_and_scheme(url: str):\n",
    "    try:\n",
    "        p = urlparse(url if '://' in url else 'http://' + url)\n",
    "        return (p.hostname or \"\").lower(), (p.scheme or \"\").lower()\n",
    "    except:\n",
    "        return \"\", \"\"\n",
    "\n",
    "def is_ip_host(host: str):\n",
    "    return bool(re.fullmatch(r\"(?:\\d{1,3}\\.){3}\\d{1,3}\", host or \"\"))\n",
    "\n",
    "def count_subdomains(host: str):\n",
    "    if not host: return 0\n",
    "    return max(0, len(host.split(\".\")) - 2)\n",
    "\n",
    "def has_double_slash_in_path(url: str):\n",
    "    return \"//\" in (urlparse(url if '://' in url else 'http://' + url).path or \"\")\n",
    "\n",
    "def has_tld_in_path(url: str):\n",
    "    path = (urlparse(url if '://' in url else 'http://' + url).path or \"\").lower()\n",
    "    return any((\".\"+tld) in path for tld in COMMON_TLDS)\n",
    "\n",
    "def has_symbols_in_domain(host: str):\n",
    "    return bool(re.search(r\"[^a-z0-9\\.-]\", host or \"\"))\n",
    "\n",
    "def domain_prefix_suffix_like_brand(host: str):\n",
    "    if not host: return False\n",
    "    first = host.split(\".\")[0]\n",
    "    return any(b in first and \"-\" in first for b in BRAND_KEYWORDS)\n",
    "\n",
    "def brand_in_path_or_subdomain(host: str, url: str):\n",
    "    text = ((host or \"\") + \" \" + (urlparse(url).path or \"\") + \" \" + (urlparse(url).query or \"\")).lower()\n",
    "    return any(b in text for b in BRAND_KEYWORDS)\n",
    "\n",
    "def digit_count(url: str):\n",
    "    return sum(c.isdigit() for c in url)\n",
    "\n",
    "def url_length(url: str):\n",
    "    return len(url)\n",
    "\n",
    "def url_entropy(url: str):\n",
    "    if not url: return 0.0\n",
    "    counts = Counter(url)\n",
    "    total = len(url)\n",
    "    return -sum((c/total) * math.log2(c/total) for c in counts.values())\n",
    "\n",
    "def fetch_html(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        r = requests.get(url, timeout=5, headers=headers)\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not fetch HTML: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_html_features(html):\n",
    "    hrefs = re.findall(r'href=[\\\"\\'](.*?)[\\\"\\']', html or '', flags=re.IGNORECASE)\n",
    "    forms = re.findall(r'<form[^>]+action=[\\\"\\'](.*?)[\\\"\\']', html or '', flags=re.IGNORECASE)\n",
    "    imgs = re.findall(r'<img[^>]+src=[\\\"\\'](.*?)[\\\"\\']', html or '', flags=re.IGNORECASE)\n",
    "    scripts = re.findall(r'<script[^>]+src=[\\\"\\'](.*?)[\\\"\\']', html or '', flags=re.IGNORECASE)\n",
    "    links_tag = re.findall(r'<link[^>]+href=[\\\"\\'](.*?)[\\\"\\']', html or '', flags=re.IGNORECASE)\n",
    "    meta_keywords = re.findall(r'<meta[^>]+name=[\\\"\\']keywords[\\\"\\'][^>]+content=[\\\"\\'](.*?)[\\\"\\']', html or '', flags=re.IGNORECASE)\n",
    "    return {'hrefs': hrefs, 'forms': forms, 'imgs': imgs, 'scripts': scripts, 'links_tag': links_tag, 'meta_keywords': meta_keywords}\n",
    "\n",
    "def abnormal_links(hrefs):\n",
    "    return any(h.strip().lower().startswith(('javascript:','mailto:','data:')) for h in hrefs)\n",
    "\n",
    "def forms_action_abnormal(forms, host):\n",
    "    for a in forms:\n",
    "        if a and host not in a and not a.startswith('/') and not a.startswith('#'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def anchors_point_elsewhere(hrefs, host):\n",
    "    count = sum(1 for h in hrefs if host and host not in h and h.startswith('http'))\n",
    "    total = max(1, len(hrefs))\n",
    "    return (count / total) > 0.5\n",
    "\n",
    "def meta_keyword_mismatch(meta_keywords, host):\n",
    "    if not meta_keywords: return False\n",
    "    for kw in meta_keywords:\n",
    "        if host and host.split('.')[0] not in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"‚úÖ Feature extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0jlJX5Fkyrk"
   },
   "source": [
    "#Veirify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3380,
     "status": "ok",
     "timestamp": 1760772458200,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "ha_CAiXZ9qBS",
    "outputId": "cd40b1dc-8843-4265-ec17-714bbda26660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying model status...\n",
      "‚úÖ Model has predict method\n",
      "‚úÖ Scaler: Loaded\n",
      "‚úÖ Tokenizer: Loaded\n",
      "‚úÖ LabelEncoder: Loaded\n",
      "   Classes: [0 1]\n",
      "üìä Model Type: BiLSTM Model (TensorFlow/Keras)\n",
      "üìê Model input layers: ['input_layer_1', 'input_layer_2']\n",
      "\n",
      "üß™ Testing prediction with sample URL...\n",
      "üéØ Using BiLSTM model prediction...\n",
      "üîç Extracted host: www.google.com\n",
      "üìä Structural features: [0, 1, 0, 0, 0, 0, 1, 22, 1, 0, 3.6635327548042547]\n",
      "üìà Scaled features shape: (1, 11)\n",
      "üî§ Tokenized sequences: [[10, 3, 3, 7, 5, 22, 2, 2, 16, 16, 16, 9, 20, 6, 6, 20, 15, 4, 9, 11, 6, 12]]\n",
      "üìè Padded sequence shape: (1, 50)\n",
      "ü§ñ Making prediction...\n",
      "üìà Prediction shape: (1, 2)\n",
      "üéØ Prediction values: [[0.8994778 0.1005222]]\n",
      "üîÆ Predicted: Safe\n",
      "üìä Confidence: 0.8995\n",
      "üî¢ Raw probabilities: Safe=0.8995, Phishing=0.1005\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Verifying model status...\")\n",
    "\n",
    "def verify_model():\n",
    "    if hasattr(model, 'predict'):\n",
    "        print(\"‚úÖ Model has predict method\")\n",
    "    else:\n",
    "        print(\"‚ùå Model missing predict method\")\n",
    "\n",
    "    if scaler is not None:\n",
    "        print(\"‚úÖ Scaler: Loaded\")\n",
    "    else:\n",
    "        print(\"‚ùå Scaler: Not loaded\")\n",
    "\n",
    "    if tokenizer is not None:\n",
    "        print(\"‚úÖ Tokenizer: Loaded\")\n",
    "    else:\n",
    "        print(\"‚ùå Tokenizer: Not loaded\")\n",
    "\n",
    "    if le is not None:\n",
    "        print(\"‚úÖ LabelEncoder: Loaded\")\n",
    "        print(f\"   Classes: {le.classes_}\")\n",
    "    else:\n",
    "        print(\"‚ùå LabelEncoder: Not loaded\")\n",
    "\n",
    "    # Check model type more accurately\n",
    "    if hasattr(model, '_is_graph_network') or hasattr(model, 'layers'):\n",
    "        print(\"üìä Model Type: BiLSTM Model (TensorFlow/Keras)\")\n",
    "        print(f\"üìê Model input layers: {[layer.name for layer in model.inputs]}\")\n",
    "    else:\n",
    "        print(\"üìä Model Type: Fallback Model\")\n",
    "\n",
    "verify_model()\n",
    "\n",
    "# Test prediction with PROPER input for BiLSTM model\n",
    "print(\"\\nüß™ Testing prediction with sample URL...\")\n",
    "test_url = \"https://www.google.com\"\n",
    "\n",
    "try:\n",
    "    # Check if this is a TensorFlow model\n",
    "    if hasattr(model, '_is_graph_network') or hasattr(model, 'layers'):\n",
    "        print(\"üéØ Using BiLSTM model prediction...\")\n",
    "\n",
    "        # Extract features like in the analyze function\n",
    "        host, scheme = parse_host_and_scheme(test_url)\n",
    "        print(f\"üîç Extracted host: {host}\")\n",
    "\n",
    "        # Prepare structural features\n",
    "        structural_features = [\n",
    "            int(is_ip_host(host)),\n",
    "            count_subdomains(host),\n",
    "            int(has_double_slash_in_path(test_url)),\n",
    "            int(has_tld_in_path(test_url)),\n",
    "            int(has_symbols_in_domain(host)),\n",
    "            int(domain_prefix_suffix_like_brand(host)),\n",
    "            int(brand_in_path_or_subdomain(host, test_url)),\n",
    "            len(test_url),\n",
    "            1 if scheme == 'https' else 0,\n",
    "            digit_count(test_url),\n",
    "            url_entropy(test_url)\n",
    "        ]\n",
    "\n",
    "        print(f\"üìä Structural features: {structural_features}\")\n",
    "\n",
    "        # Scale the features\n",
    "        struct_feat = scaler.transform([structural_features])\n",
    "        print(f\"üìà Scaled features shape: {struct_feat.shape}\")\n",
    "\n",
    "        # Prepare sequence features\n",
    "        if tokenizer:\n",
    "            sequences = tokenizer.texts_to_sequences([test_url])\n",
    "            print(f\"üî§ Tokenized sequences: {sequences}\")\n",
    "            seq = pad_sequences(sequences, maxlen=maxlen)\n",
    "            print(f\"üìè Padded sequence shape: {seq.shape}\")\n",
    "        else:\n",
    "            print(\"‚ùå Tokenizer not available\")\n",
    "            seq = np.zeros((1, maxlen))  # Fallback\n",
    "\n",
    "        # Make prediction\n",
    "        print(\"ü§ñ Making prediction...\")\n",
    "        prediction = model.predict([seq, struct_feat], verbose=0)\n",
    "        print(f\"üìà Prediction shape: {prediction.shape}\")\n",
    "        print(f\"üéØ Prediction values: {prediction}\")\n",
    "\n",
    "        # Decode prediction\n",
    "        predicted_class_idx = np.argmax(prediction[0])\n",
    "        predicted_class = le.inverse_transform([predicted_class_idx])[0]\n",
    "        confidence = np.max(prediction[0])\n",
    "\n",
    "        class_names = {0: 'Safe', 1: 'Phishing'}\n",
    "        human_readable_class = class_names.get(predicted_class_idx, predicted_class)\n",
    "\n",
    "        print(f\"üîÆ Predicted: {human_readable_class}\")\n",
    "        print(f\"üìä Confidence: {confidence:.4f}\")\n",
    "        print(f\"üî¢ Raw probabilities: Safe={prediction[0][0]:.4f}, Phishing={prediction[0][1]:.4f}\")\n",
    "\n",
    "    else:\n",
    "        print(\"üéØ Using fallback model prediction...\")\n",
    "        if hasattr(model, 'predict_proper'):\n",
    "            prediction = model.predict_proper(test_url)\n",
    "        else:\n",
    "            safe_prob = 0.8 if 'google' in test_url else 0.4\n",
    "            phishing_prob = 1 - safe_prob\n",
    "            prediction = np.array([[safe_prob, phishing_prob]])\n",
    "\n",
    "        print(f\"üìà Prediction values: {prediction}\")\n",
    "        safe_prob = prediction[0][0]\n",
    "        phishing_prob = prediction[0][1]\n",
    "        label = \"Safe\" if safe_prob > phishing_prob else \"Phishing\"\n",
    "        print(f\"üîÆ Predicted: {label}\")\n",
    "        print(f\"üìä Probabilities: Safe={safe_prob:.4f}, Phishing={phishing_prob:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Prediction test failed: {e}\")\n",
    "    import traceback\n",
    "    print(f\"üîç Detailed error: {traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0cSEd-s0-Mf"
   },
   "source": [
    "#Load data from phishtank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1613,
     "status": "ok",
     "timestamp": 1760772459814,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "9jktYtwWt3yR",
    "outputId": "48267419-2fee-4e1f-e128-b75ab29ce9be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading PhishTank dataset...\n",
      "[INFO] Loaded 49525 phishing URLs from PhishTank.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import csv\n",
    "import io\n",
    "import requests\n",
    "\n",
    "def load_phishtank_database():\n",
    "    print(\"[INFO] Downloading PhishTank dataset...\")\n",
    "    url = \"http://data.phishtank.com/data/online-valid.csv.gz\"\n",
    "    r = requests.get(url, timeout=15)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    data = gzip.decompress(r.content)\n",
    "    csv_data = csv.DictReader(io.StringIO(data.decode()))\n",
    "\n",
    "    urls = {row['url'] for row in csv_data}\n",
    "    print(f\"[INFO] Loaded {len(urls)} phishing URLs from PhishTank.\")\n",
    "    return urls\n",
    "\n",
    "phishtank_cache = load_phishtank_database()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vU_HAVxm1BKi"
   },
   "source": [
    "#load data and setup cache interval openphish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1760772459822,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "yMLFsh8WyiFM"
   },
   "outputs": [],
   "source": [
    "OPENPHISH_FEED_URL = \"https://openphish.com/feed.txt\"\n",
    "_openphish_cache = None\n",
    "_openphish_last_loaded = None\n",
    "\n",
    "def load_openphish_database(feed_url=OPENPHISH_FEED_URL, timeout=20):\n",
    "\n",
    "    global _openphish_cache, _openphish_last_loaded\n",
    "    try:\n",
    "        print(f\"[INFO] Downloading OpenPhish feed from {feed_url} ...\")\n",
    "        r = requests.get(feed_url, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        text = r.text\n",
    "        urls = set()\n",
    "        for line in text.splitlines():\n",
    "            u = line.strip()\n",
    "            if not u:\n",
    "                continue\n",
    "            u_norm = normalize_url_for_lookup(u)\n",
    "            urls.add(u_norm)\n",
    "        _openphish_cache = urls\n",
    "        _openphish_last_loaded = time.time()\n",
    "        print(f\"[INFO] Loaded {len(urls)} URLs from OpenPhish.\")\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load OpenPhish feed: {e}\")\n",
    "        return _openphish_cache if _openphish_cache is not None else set()\n",
    "\n",
    "def normalize_url_for_lookup(url):\n",
    "\n",
    "    u = url.strip().lower()\n",
    "    if '#' in u:\n",
    "        u = u.split('#', 1)[0]\n",
    "    if u.endswith('/'):\n",
    "        u = u[:-1]\n",
    "    return u\n",
    "\n",
    "def ensure_openphish_loaded(force_reload=False):\n",
    "\n",
    "    global _openphish_cache, _openphish_last_loaded\n",
    "    if _openphish_cache is None or force_reload:\n",
    "        load_openphish_database()\n",
    "    return _openphish_cache\n",
    "\n",
    "def refresh_openphish_cache_interval(hours=24):\n",
    "    global _openphish_last_loaded\n",
    "    if _openphish_last_loaded is None:\n",
    "        load_openphish_database()\n",
    "        return\n",
    "    age = (time.time() - _openphish_last_loaded) / 3600.0\n",
    "    if age >= hours:\n",
    "        print(f\"[INFO] OpenPhish cache older than {hours}h (age={age:.2f}h). Reloading...\")\n",
    "        load_openphish_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fgcj8R9mVYQr"
   },
   "source": [
    "#Analysis extraction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1760772460007,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "IKf0gfAFVY42",
    "outputId": "61b05c02-7566-4ccc-8f9f-e00a86269b93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analysis functions defined!\n"
     ]
    }
   ],
   "source": [
    "import tldextract\n",
    "import datetime\n",
    "import whois\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "\n",
    "def digit_count(url):\n",
    "    return sum(c.isdigit() for c in url)\n",
    "\n",
    "def url_length(url):\n",
    "    return len(url)\n",
    "\n",
    "def url_entropy(url):\n",
    "    # Shannon entropy\n",
    "    prob = [float(url.count(c)) / len(url) for c in set(url)]\n",
    "    return -sum(p * math.log2(p) for p in prob)\n",
    "\n",
    "def is_shortened_url(url):\n",
    "    shortened_domains = [\"bit.ly\", \"tinyurl.com\", \"goo.gl\", \"t.co\", \"ow.ly\"]\n",
    "    for d in shortened_domains:\n",
    "        if d in url:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def suspicious_tld(host):\n",
    "    suspicious = [\"xyz\", \"top\", \"club\", \"online\"]\n",
    "    ext = tldextract.extract(host).suffix\n",
    "    return ext in suspicious\n",
    "\n",
    "def domain_age_months(host):\n",
    "    try:\n",
    "        w = whois.whois(host)\n",
    "        creation_date = w.creation_date\n",
    "        if isinstance(creation_date, list):\n",
    "            creation_date = creation_date[0]\n",
    "        if creation_date is None:\n",
    "            return 0\n",
    "        delta = datetime.datetime.now() - creation_date\n",
    "        return delta.days // 30\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def homoglyph_in_domain(host):\n",
    "    pattern = r\"[0-9@!$]\"\n",
    "    return bool(re.search(pattern, host))\n",
    "\n",
    "\n",
    "def in_phishtank(url):\n",
    "\n",
    "    normalized_url = url.strip().lower()\n",
    "    return normalized_url in phishtank_cache\n",
    "\n",
    "def in_openphish(url):\n",
    "\n",
    "    ensure_openphish_loaded()\n",
    "    if not _openphish_cache:\n",
    "        return False\n",
    "    u_norm = normalize_url_for_lookup(url)\n",
    "    return u_norm in _openphish_cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def phishing_score(url, html):\n",
    "    host, scheme = parse_host_and_scheme(url)\n",
    "    features = extract_html_features(html)\n",
    "    score = 0\n",
    "    reasons = []\n",
    "\n",
    "    # ---- Rule-based scoring ----\n",
    "    if is_ip_host(host):\n",
    "        score += 2; reasons.append(\"Host is an IP address\")\n",
    "    if count_subdomains(host) > 2:\n",
    "        score += 1; reasons.append(\"Too many subdomains\")\n",
    "    if has_symbols_in_domain(host):\n",
    "        score += 1; reasons.append(\"Suspicious symbols in domain\")\n",
    "    if domain_prefix_suffix_like_brand(host):\n",
    "        score += 2; reasons.append(\"Domain mimics brand with hyphens\")\n",
    "    if brand_in_path_or_subdomain(host, url):\n",
    "        score += 1; reasons.append(\"Brand keywords in path or subdomain\")\n",
    "    if has_double_slash_in_path(url):\n",
    "        score += 1; reasons.append(\"Double slash in path\")\n",
    "    if has_tld_in_path(url):\n",
    "        score += 1; reasons.append(\"TLD in path\")\n",
    "    if abnormal_links(features['hrefs']):\n",
    "        score += 1; reasons.append(\"Suspicious links found\")\n",
    "    if forms_action_abnormal(features['forms'], host):\n",
    "        score += 2; reasons.append(\"Suspicious form actions\")\n",
    "    if anchors_point_elsewhere(features['hrefs'], host):\n",
    "        score += 1; reasons.append(\"Many anchors point elsewhere\")\n",
    "    if meta_keyword_mismatch(features['meta_keywords'], host):\n",
    "        score += 1; reasons.append(\"Meta keywords mismatch\")\n",
    "\n",
    "    # ---- URL-based scoring ----\n",
    "    dcount = digit_count(url)\n",
    "    ulen = url_length(url)\n",
    "    uentropy = url_entropy(url)\n",
    "\n",
    "    if dcount > 5:\n",
    "        score += 1; reasons.append(f\"Too many digits ({dcount})\")\n",
    "    if ulen > 75:\n",
    "        score += 1; reasons.append(f\"URL too long ({ulen} chars)\")\n",
    "    if uentropy > 4.0:\n",
    "        score += 1; reasons.append(f\"High URL entropy ({uentropy:.2f})\")\n",
    "\n",
    "    if is_shortened_url(url):\n",
    "        score += 1; reasons.append(\"Uses URL shortening service\")\n",
    "    if suspicious_tld(host):\n",
    "        score += 1; reasons.append(\"Suspicious TLD\")\n",
    "    if domain_age_months(host) < 6:\n",
    "        score += 1; reasons.append(\"Domain is newly registered (<6 months)\")\n",
    "    if homoglyph_in_domain(host):\n",
    "        score += 2; reasons.append(\"Homoglyph/symbol characters in domain\")\n",
    "\n",
    "    # ---- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö PhishTank dataset ----\n",
    "    in_phishtank_flag = in_phishtank(url)\n",
    "    if in_phishtank_flag:\n",
    "        score += 5\n",
    "        reasons.append(\"URL found in PhishTank blacklist\")\n",
    "\n",
    "    try:\n",
    "        in_openphish_flag = in_openphish(url)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] OpenPhish lookup failed: {e}\")\n",
    "        in_openphish_flag = False\n",
    "\n",
    "    if in_openphish_flag:\n",
    "        score += 5\n",
    "        reasons.append(\"URL found in OpenPhish feed\")\n",
    "\n",
    "    # update features\n",
    "    features.update({\n",
    "        \"digit_count\": dcount,\n",
    "        \"url_length\": ulen,\n",
    "        \"url_entropy\": uentropy,\n",
    "        \"in_phishtank\": in_phishtank_flag,\n",
    "        \"in_openphish\": in_openphish_flag\n",
    "    })\n",
    "\n",
    "    return score, reasons, features, host, scheme\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_url(url):\n",
    "    host, scheme = parse_host_and_scheme(url)\n",
    "\n",
    "    # Store URL for demo model\n",
    "    if hasattr(model, 'last_url'):\n",
    "        model.last_url = url\n",
    "\n",
    "    if scaler is None or tokenizer is None or le is None:\n",
    "        pred = model.predict([None])[0]\n",
    "        label = \"Likely Safe\" if pred[0] > pred[1] else \"Likely Phishing\"\n",
    "        return label, pred\n",
    "\n",
    "    # Actual prediction with real model\n",
    "    try:\n",
    "        struct_feat = scaler.transform([[\n",
    "            int(is_ip_host(host)),\n",
    "            count_subdomains(host),\n",
    "            int(has_double_slash_in_path(url)),\n",
    "            int(has_tld_in_path(url)),\n",
    "            int(has_symbols_in_domain(host)),\n",
    "            int(domain_prefix_suffix_like_brand(host)),\n",
    "            int(brand_in_path_or_subdomain(host, url)),\n",
    "            len(url),\n",
    "            1 if scheme == 'https' else 0,\n",
    "            digit_count(url),\n",
    "            url_entropy(url)\n",
    "        ]])\n",
    "\n",
    "        seq = pad_sequences(tokenizer.texts_to_sequences([url]), maxlen=maxlen)\n",
    "        pred = model.predict([seq, struct_feat])[0]\n",
    "        label = le.inverse_transform([np.argmax(pred)])[0]\n",
    "        return label, pred\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Prediction error: {e}\")\n",
    "        pred = model.predict([None])[0]\n",
    "        label = \"Likely Safe\" if pred[0] > pred[1] else \"Likely Phishing\"\n",
    "        return label, pred\n",
    "\n",
    "print(\"‚úÖ Analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9B7j9MfqVecH"
   },
   "source": [
    "#Set API Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1760772460049,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "Y9gsra5fVe4L",
    "outputId": "df5eb777-a2ab-4946-eb41-5e70cbc34d85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API endpoints defined!\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class URLRequest(BaseModel):\n",
    "    url: str\n",
    "    call_llm: bool = True\n",
    "\n",
    "def clean_numpy(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_numpy(x) for x in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64)):\n",
    "        return float(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "@app.post(\"/analyze\")\n",
    "def analyze(request: URLRequest):\n",
    "    url = request.url\n",
    "    print(f\"üîç Analyzing URL: {url}\")\n",
    "\n",
    "    html = fetch_html(url)\n",
    "    score, reasons, features, host, scheme = phishing_score(url, html)\n",
    "\n",
    "    # BiLSTM prediction\n",
    "    bilstm_label, bilstm_prob_array = predict_url(url)\n",
    "    label_idx = np.argmax(bilstm_prob_array)\n",
    "    bilstm_prob = float(bilstm_prob_array[label_idx])\n",
    "    score = int(score)\n",
    "\n",
    "    llm_result = None\n",
    "    if request.call_llm and client:\n",
    "        prompt = f\"\"\"\n",
    "Analyze this URL for phishing potential:\n",
    "\n",
    "URL: {url}\n",
    "Host: {host}\n",
    "Scheme: {scheme}\n",
    "\n",
    "AI Prediction: {bilstm_label} (confidence={bilstm_prob:.2f})\n",
    "Rule-based Risk Score: {score}/15\n",
    "\n",
    "Triggered Alerts:\n",
    "- {\"\\n- \".join(reasons)}\n",
    "\n",
    "Technical Features:\n",
    "- Digit count: {features.get('digit_count')}\n",
    "- URL length: {features.get('url_length')}\n",
    "- URL entropy: {features.get('url_entropy'):.2f}\n",
    "- External links: {len(features.get('hrefs', []))}\n",
    "- Images: {len(features.get('imgs', []))}\n",
    "- Scripts: {len(features.get('scripts', []))}\n",
    "- Forms: {len(features.get('forms', []))}\n",
    "\n",
    "Provide a concise analysis (2-3 sentences) and final verdict as 'Likely Phishing' or 'Likely Safe'.\n",
    "Return JSON format:\n",
    "{{\n",
    "    \"verdict\": \"...\",\n",
    "    \"reason_list\": [\"...\",\"...\"],\n",
    "    \"summary\": \"...\"\n",
    "}}\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\":\"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            raw_text = response.choices[0].message.content.strip()\n",
    "\n",
    "            # Clean JSON response\n",
    "            if raw_text.startswith(\"```json\"):\n",
    "                raw_text = raw_text[7:-3].strip()\n",
    "            elif raw_text.startswith(\"```\"):\n",
    "                raw_text = raw_text[3:-3].strip()\n",
    "\n",
    "            llm_result = json.loads(raw_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            llm_result = {\n",
    "                \"verdict\": \"Analysis Error\",\n",
    "                \"reason_list\": [f\"LLM processing failed: {str(e)}\"],\n",
    "                \"summary\": \"Could not complete AI analysis\"\n",
    "            }\n",
    "    elif request.call_llm and not client:\n",
    "        llm_result = {\n",
    "            \"verdict\": \"No API Key\",\n",
    "            \"reason_list\": [\"OpenAI API key not provided\"],\n",
    "            \"summary\": \"LLM analysis disabled. Please provide an OpenAI API key.\"\n",
    "        }\n",
    "\n",
    "    response = {\n",
    "        \"url\": url,\n",
    "        \"score\": score,\n",
    "        \"reasons\": reasons,\n",
    "        \"features\": features,\n",
    "        \"bilstm_label\": bilstm_label,\n",
    "        \"bilstm_prob\": bilstm_prob,\n",
    "        \"llm_result\": llm_result,\n",
    "        \"host\": host,\n",
    "        \"scheme\": scheme,\n",
    "        \"in_phishtank\": features.get(\"in_phishtank\", False),\n",
    "    }\n",
    "\n",
    "    # convert numpy type to Python type recursive\n",
    "    response_clean = clean_numpy(response)\n",
    "    return response_clean\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Phishing URL Analyzer API is running!\", \"status\": \"active\"}\n",
    "\n",
    "\n",
    "@app.get(\"/test\")\n",
    "def test_analysis():\n",
    "    test_urls = [\"https://www.google.com\", \"https://www.github.com\", \"http://example.com\"]\n",
    "    results = []\n",
    "    for test_url in test_urls:\n",
    "        try:\n",
    "            result = analyze(URLRequest(url=test_url, call_llm=False))\n",
    "            results.append({\n",
    "                \"url\": test_url,\n",
    "                \"bilstm_label\": result[\"bilstm_label\"],\n",
    "                \"score\": result[\"score\"]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\"url\": test_url, \"error\": f\"Analysis failed: {str(e)}\"})\n",
    "    return {\n",
    "        \"test_results\": results,\n",
    "        \"message\": \"API is working!\",\n",
    "        \"web_interface\": f\"{colab_public_url}/web\"\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ API endpoints defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnR_hzd7Vjch"
   },
   "source": [
    "#Create web interface in \"/web\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1760772460083,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "rNQ0kl6SVj-D"
   },
   "outputs": [],
   "source": [
    "@app.get(\"/web\", response_class=HTMLResponse)\n",
    "def web_interface():\n",
    "    return \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>Phishing URL Analyzer</title>\n",
    "<style>\n",
    "* { margin:0; padding:0; box-sizing:border-box; }\n",
    "body { font-family:'Segoe UI',Tahoma,Verdana,sans-serif; background:linear-gradient(135deg,#667eea,#764ba2); min-height:100vh; padding:20px; }\n",
    ".container { max-width:900px; margin:0 auto; background:white; border-radius:15px; box-shadow:0 20px 40px rgba(0,0,0,0.1); overflow:hidden; }\n",
    ".header { background:linear-gradient(135deg,#007BFF,#0056b3); color:white; padding:30px; text-align:center; }\n",
    ".header h1 { font-size:2.2em; margin-bottom:10px; }\n",
    ".demo-notice { background:#fff3cd; color:#856404; padding:10px; border-radius:5px; margin-top:10px; }\n",
    ".input-section { padding:30px; background:#f8f9fa; border-bottom:1px solid #e9ecef; }\n",
    ".input-group { display:flex; gap:10px; }\n",
    "#urlInput { flex:1; padding:15px; border:2px solid #e9ecef; border-radius:8px; font-size:16px; }\n",
    "button { padding:15px 25px; background:#007BFF; color:white; border:none; border-radius:8px; font-size:16px; cursor:pointer; }\n",
    "button:hover { background:#0056b3; }\n",
    ".loading { display:none; text-align:center; padding:20px; color:#007BFF; }\n",
    ".spinner { border:4px solid #f3f3f3; border-top:4px solid #007BFF; border-radius:50%; width:40px; height:40px; animation:spin 1s linear infinite; margin:0 auto 10px; }\n",
    "@keyframes spin { 0%{transform:rotate(0);}100%{transform:rotate(360deg);} }\n",
    ".result-section { padding:30px; }\n",
    ".result-card { background:white; border-radius:10px; padding:25px; margin-bottom:20px; box-shadow:0 5px 15px rgba(0,0,0,0.1); border-left:5px solid #007BFF; }\n",
    ".section-title { font-size:1.3em; font-weight:bold; color:#333; margin-bottom:10px; }\n",
    ".reason-list, .feature-list { list-style:none; padding:0; }\n",
    ".reason-list li, .feature-list li { padding:6px 0; border-bottom:1px solid #f8f9fa; }\n",
    ".error-message { background:#f8d7da; color:#721c24; padding:15px; border-radius:8px; margin:20px 0; }\n",
    ".url-display { background:#e7f3ff; padding:15px; border-radius:8px; margin-bottom:20px; word-break:break-all; }\n",
    "\n",
    "/* üé® Gradient risk bar styles */\n",
    ".probability-bar {\n",
    "  background: #e9ecef;\n",
    "  border-radius: 10px;\n",
    "  height: 25px;\n",
    "  position: relative;\n",
    "  overflow: hidden;\n",
    "}\n",
    "\n",
    ".probability-fill {\n",
    "  height: 100%;\n",
    "  transition: width .5s, background-color .5s;\n",
    "  border-radius: 10px 0 0 10px;\n",
    "}\n",
    "\n",
    ".probability-labels {\n",
    "  display: flex;\n",
    "  justify-content: space-between;\n",
    "  font-size: 0.9em;\n",
    "  color: #555;\n",
    "  margin-top: 5px;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"container\">\n",
    "    <div class=\"header\">\n",
    "        <h1>üîç Phishing URL Analyzer</h1>\n",
    "        <p>Analyze URLs for potential phishing threats using AI and rules</p>\n",
    "        <div class=\"demo-notice\"><strong>Demo Mode:</strong> Upload model files for real predictions.</div>\n",
    "    </div>\n",
    "    <div class=\"input-section\">\n",
    "        <div class=\"input-group\">\n",
    "            <input type=\"text\" id=\"urlInput\" placeholder=\"Enter URL (https://example.com)\">\n",
    "            <button onclick=\"analyzeUrl()\">Analyze URL</button>\n",
    "        </div>\n",
    "        <div id=\"loading\" class=\"loading\">\n",
    "            <div class=\"spinner\"></div>\n",
    "            <p>Analyzing URL...</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div id=\"resultSection\" class=\"result-section\">\n",
    "        <div class=\"result-card\">\n",
    "            <div class=\"section-title\">üéØ How to Use</div>\n",
    "            <p>Enter a URL to analyze its phishing potential using AI + rules.</p>\n",
    "            <p><strong>Examples:</strong> https://www.google.com, https://github.com</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "function analyzeUrl() {\n",
    "    const url = document.getElementById('urlInput').value.trim();\n",
    "    const loading = document.getElementById('loading');\n",
    "    const resultSection = document.getElementById('resultSection');\n",
    "    const button = document.querySelector('button');\n",
    "\n",
    "    if (!url) return alert('Please enter a URL.');\n",
    "    if (!url.startsWith('http')) return alert('URL must start with http:// or https://');\n",
    "\n",
    "    loading.style.display = 'block';\n",
    "    resultSection.innerHTML = '';\n",
    "    button.disabled = true;\n",
    "\n",
    "    fetch('/analyze', {\n",
    "        method: 'POST',\n",
    "        headers: { 'Content-Type': 'application/json' },\n",
    "        body: JSON.stringify({ url: url, call_llm: true })\n",
    "    })\n",
    "    .then(r => r.json())\n",
    "    .then(data => displayResults(data))\n",
    "    .catch(err => {\n",
    "        resultSection.innerHTML = `<div class=\"error-message\"><strong>Error:</strong> ${err}</div>`;\n",
    "    })\n",
    "    .finally(() => { loading.style.display = 'none'; button.disabled = false; });\n",
    "}\n",
    "\n",
    "function getColor(prob) {\n",
    "    if (prob < 0.65) return \"#28a745\";   // Green (Safe)\n",
    "    if (prob < 0.75) return \"#ffc107\";   // Yellow\n",
    "    if (prob < 0.85) return \"#fd7e14\";   // Orange\n",
    "    return \"#dc3545\";                    // Red (Phishing)\n",
    "}\n",
    "\n",
    "\n",
    "function displayResults(data) {\n",
    "    const resultSection = document.getElementById('resultSection');\n",
    "\n",
    "    const probNum = Number(data.bilstm_prob) || 0;\n",
    "    const prob = (probNum * 100).toFixed(1);\n",
    "    const score = Number(data.score) || 0;\n",
    "    const risk = score < 3 ? 'Low' : score < 7 ? 'Medium' : 'High';\n",
    "    const llm = data.llm_result;\n",
    "\n",
    "    const html = `\n",
    "        <div class=\"result-card\">\n",
    "            <div class=\"section-title\">üìã URL Info</div>\n",
    "            <div class=\"url-display\"><strong>URL:</strong> ${data.url}</div>\n",
    "            <p><strong>Host:</strong> ${data.host || 'N/A'}</p>\n",
    "            <p><strong>Scheme:</strong> ${data.scheme || 'N/A'}</p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"result-card\">\n",
    "            <div class=\"section-title\">ü§ñ AI Prediction</div>\n",
    "            <div class=\"probability-bar\">\n",
    "                <div class=\"probability-fill\" style=\"width:${prob}%; background-color:${getColor(probNum)}\"></div>\n",
    "            </div>\n",
    "            <div class=\"probability-labels\">\n",
    "                <span>Safe</span><span>Phishing</span>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"result-card\">\n",
    "            <div class=\"section-title\">‚öñÔ∏è Rule-based Analysis</div>\n",
    "            <p><strong>Risk Level:</strong> ${risk} (${score}/15)</p>\n",
    "            ${data.reasons && data.reasons.length ?\n",
    "                `<ul class=\"reason-list\">${data.reasons.map(r=>`<li>${r}</li>`).join('')}</ul>` :\n",
    "                '<p>‚úÖ No suspicious indicators found.</p>'}\n",
    "        </div>\n",
    "\n",
    "        <div class=\"result-card\">\n",
    "            <div class=\"section-title\">üìä Extracted Features</div>\n",
    "            <ul class=\"feature-list\">\n",
    "                <li><strong>URL Length:</strong> ${data.features?.url_length || 0}</li>\n",
    "                <li><strong>Digit Count:</strong> ${data.features?.digit_count || 0}</li>\n",
    "                <li><strong>Entropy:</strong> ${(data.features?.url_entropy||0).toFixed(2)}</li>\n",
    "                <li><strong>Links:</strong> ${data.features?.hrefs?.length || 0}</li>\n",
    "                <li><strong>Images:</strong> ${data.features?.imgs?.length || 0}</li>\n",
    "                <li><strong>Scripts:</strong> ${data.features?.scripts?.length || 0}</li>\n",
    "                <li><strong>Forms:</strong> ${data.features?.forms?.length || 0}</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "\n",
    "        ${llm ? `\n",
    "        <div class=\"result-card\">\n",
    "            <div class=\"section-title\">üß† LLM Analysis</div>\n",
    "            <p><strong>Verdict:</strong> ${llm.verdict}</p>\n",
    "            <p><strong>Summary:</strong> ${llm.summary}</p>\n",
    "            ${llm.reason_list?.length ?\n",
    "                `<ul class=\"reason-list\">${llm.reason_list.map(r=>`<li>${r}</li>`).join('')}</ul>` : ''}\n",
    "        </div>` : ''}\n",
    "\n",
    "        <!-- ‚úÖ ‡∏à‡∏∏‡∏î‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• PhishTank -->\n",
    "        <div id=\"phishTankResult\" class=\"result-card\"></div>\n",
    "    `;\n",
    "\n",
    "    resultSection.innerHTML = html;\n",
    "\n",
    "    // ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à PhishTank ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "    if (data.in_phishtank) {\n",
    "        document.getElementById(\"phishTankResult\").innerHTML =\n",
    "            \"üß† PhishTank Status: <span style='color:red;'>‚ö†Ô∏è Found in PhishTank</span>\";\n",
    "    } else {\n",
    "        document.getElementById(\"phishTankResult\").innerHTML =\n",
    "            \"üß† PhishTank Status: <span style='color:green;'>‚úÖ Not listed in PhishTank</span>\";\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "document.getElementById('urlInput').addEventListener('keypress', e => {\n",
    "    if (e.key === 'Enter') analyzeUrl();\n",
    "});\n",
    "</script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9APX9eWWBGD"
   },
   "source": [
    "#Start ngrok server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyngrok --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1760772461017,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "985W2uAC1sqE",
    "outputId": "fd20e086-4fcd-4658-df50-7033581194f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok authtoken #authToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14993,
     "status": "ok",
     "timestamp": 1760772476031,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "hRqeXZIWWBZd",
    "outputId": "bd38aaad-1768-4602-9b2c-3a1523915727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Server with Ngrok...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [8327]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Ngrok Public URL: NgrokTunnel: \"https://unprivileged-multinucleolate-dahlia.ngrok-free.dev\" -> \"http://localhost:8000\"\n",
      "‚è≥ Waiting for server to start...\n",
      "\n",
      "============================================================\n",
      "üéØ PHISHING URL ANALYZER - READY!\n",
      "============================================================\n",
      "üåê Web Interface: NgrokTunnel: \"https://unprivileged-multinucleolate-dahlia.ngrok-free.dev\" -> \"http://localhost:8000\"/web\n",
      "üîß API Test: NgrokTunnel: \"https://unprivileged-multinucleolate-dahlia.ngrok-free.dev\" -> \"http://localhost:8000\"/test\n",
      "üí° Open the Web Interface URL above in a NEW TAB!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting Server with Ngrok...\")\n",
    "\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "import time\n",
    "import requests\n",
    "\n",
    "ngrok.kill()\n",
    "\n",
    "public_url = ngrok.connect(8000, bind_tls=True)\n",
    "print(f\"üåê Ngrok Public URL: {public_url}\")\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "\n",
    "# Start server in background\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"‚è≥ Waiting for server to start...\")\n",
    "time.sleep(8)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ PHISHING URL ANALYZER - READY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üåê Web Interface: {public_url}/web\")\n",
    "print(f\"üîß API Test: {public_url}/test\")\n",
    "print(\"üí° Open the Web Interface URL above in a NEW TAB!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr8BLWP8lNFn"
   },
   "source": [
    "#Test ngrok server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1760772509255,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "b661f4a2",
    "outputId": "b0346ad3-cb45-49a3-e2a9-6e037c8c7b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing with Ngrok...\n",
      "üåê Using Ngrok URL: https://unprivileged-multinucleolate-dahlia.ngrok-free.dev\n",
      "üîç Testing server connectivity...\n",
      "INFO:     34.26.122.64:0 - \"GET / HTTP/1.1\" 200 OK\n",
      "‚úÖ Server is running via Ngrok!\n",
      "üîç Testing analysis for: https://www.google.com/\n",
      "üîç Analyzing URL: https://www.google.com/\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "INFO:     34.26.122.64:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
      "‚úÖ Analysis successful!\n",
      "   üìä Result: 0\n",
      "   üéØ Confidence: 98.77%\n",
      "   ‚öñÔ∏è Risk Score: 2/15\n",
      "\n",
      "üéâ System is working! Open the web interface:\n",
      "üëâ https://unprivileged-multinucleolate-dahlia.ngrok-free.dev/web\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üß™ Testing with Ngrok...\")\n",
    "\n",
    "def test_with_ngrok():\n",
    "    try:\n",
    "        # Get the ngrok URL\n",
    "        from pyngrok import ngrok\n",
    "        tunnels = ngrok.get_tunnels()\n",
    "        if tunnels:\n",
    "            public_url = tunnels[0].public_url\n",
    "            print(f\"üåê Using Ngrok URL: {public_url}\")\n",
    "        else:\n",
    "            print(\"‚ùå No ngrok tunnel found\")\n",
    "            return False\n",
    "\n",
    "        # Test basic connectivity\n",
    "        print(\"üîç Testing server connectivity...\")\n",
    "        response = requests.get(f\"{public_url}/\", timeout=15)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Server is running via Ngrok!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Server error: {response.status_code}\")\n",
    "            return False\n",
    "\n",
    "        # Test analysis\n",
    "        test_url = \"https://www.google.com/\"\n",
    "        print(f\"üîç Testing analysis for: {test_url}\")\n",
    "\n",
    "        response = requests.post(\n",
    "            f\"{public_url}/analyze\",\n",
    "            json={\"url\": test_url, \"call_llm\": False},\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            results = response.json()\n",
    "            print(\"‚úÖ Analysis successful!\")\n",
    "            print(f\"   üìä Result: {results['bilstm_label']}\")\n",
    "            print(f\"   üéØ Confidence: {results['bilstm_prob']:.2%}\")\n",
    "            print(f\"   ‚öñÔ∏è Risk Score: {results['score']}/15\")\n",
    "            print(f\"\\nüéâ System is working! Open the web interface:\")\n",
    "            print(f\"üëâ {public_url}/web\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Analysis failed: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run test\n",
    "test_with_ngrok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1760772477111,
     "user": {
      "displayName": "polakrit krajaisri",
      "userId": "09656006660873636975"
     },
     "user_tz": -420
    },
    "id": "WyzjQP-bLf73"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
